{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Overfitting with Dropouts\n",
    "1. How dropout works\n",
    "2. Dropout on input layers\n",
    "3. Dropout on hidden layers\n",
    "\n",
    "Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting1. Dropout is a technique where randomly selected neurons are ignored during training. They are dropped-out randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "Dropout is only used during the training of a model and is not used when evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 82.14% (7.30%)\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproduction\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv(\"Accessory_files/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split to X and Y\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y = dataset[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal',activation = 'relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay = 0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators=[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Dropouts\n",
    "Dropout can be applied to input neurons called the visible layer. In the example below we add a new Dropout layer between the input (or visible layer) and the first hidden layer. The dropout rate is set to 20%, meaning one in five inputs will be randomly excluded from each update cycle.\n",
    "Additionally, as recommended in the original paper on dropout, a constraint is imposed on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 3. This is done by setting the kernel constraint argument on the Dense class when constructing the layers. The learning rate was lifted by one order of magnitude and the momentum was increased to 0.9. These increases in the learning rate were also recommended in the original dropout paper. Continuing on from the baseline example above, the code below exercises the same network with input dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 87.49% (4.87%)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "# set random seed for reproduction\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv(\"Accessory_files/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split to X and Y\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y = dataset[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60,)))\n",
    "    model.add(Dense(60, kernel_initializer='normal',activation = 'relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1,kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, decay = 0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators=[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 82.68% (6.48%)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "# set random seed for reproduction\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv(\"Accessory_files/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split to X and Y\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y = dataset[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim = 60, kernel_initializer='normal',activation = 'relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, decay = 0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators=[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for using dropouts\n",
    "1. choose 20% to 50%\n",
    "2. large network\n",
    "3. dropout on both the input and the hidden layers\n",
    "4. increase learning rate and use a high momentum value\n",
    "5. Constrain the size of the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 84.59% (7.11%)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "# set random seed for reproduction\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv(\"Accessory_files/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split to X and Y\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y = dataset[:, 60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2,input_shape=(60,)))\n",
    "    model.add(Dense(60, kernel_initializer='normal',activation = 'relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay = 0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators=[]\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
