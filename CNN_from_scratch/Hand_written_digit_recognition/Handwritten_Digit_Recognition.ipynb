{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following CNN trains data on the famous MNIST dataset for handwritten_Digit_Recognition\n",
    "Each image is of size 28 x 28\n",
    "60,000 training example + 10,000 test example\n",
    "\n",
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1140f0ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the MNIST data set in Keras\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "(X_train, y_train), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "#plot 4 of the many images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[5], cmap=plt.get_cmap('gray')) \n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[10], cmap=plt.get_cmap('gray')) \n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[15], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils # one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') \n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# input normalization\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode the output\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network model, fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels,input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes,kernel_initializer='normal', activation='softmax'))\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 0.2784 - acc: 0.9209 - val_loss: 0.1411 - val_acc: 0.9572\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.1119 - acc: 0.9676 - val_loss: 0.0919 - val_acc: 0.9703\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.0719 - acc: 0.9796 - val_loss: 0.0787 - val_acc: 0.9771\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0507 - acc: 0.9855 - val_loss: 0.0743 - val_acc: 0.9768\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0374 - acc: 0.9892 - val_loss: 0.0668 - val_acc: 0.9794\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.0271 - acc: 0.9927 - val_loss: 0.0632 - val_acc: 0.9806\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0209 - acc: 0.9948 - val_loss: 0.0626 - val_acc: 0.9809\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0620 - val_acc: 0.9799\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.0107 - acc: 0.9980 - val_loss: 0.0588 - val_acc: 0.9807\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.0080 - acc: 0.9985 - val_loss: 0.0585 - val_acc: 0.9816\n",
      "Baseline Error: 1.84%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = base_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional libraries\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "# Sets the value of the image dimension ordering convention to theano\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [channels][width][height]. In the case of RGB, the first dimension channels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In the case of MNIST where the channels values are gray scale, the pixel dimension is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32') \n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(5,5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 137s - loss: 0.2324 - acc: 0.9340 - val_loss: 0.0813 - val_acc: 0.9744\n",
      "Epoch 2/10\n",
      " - 133s - loss: 0.0736 - acc: 0.9783 - val_loss: 0.0463 - val_acc: 0.9843\n",
      "Epoch 3/10\n",
      " - 135s - loss: 0.0530 - acc: 0.9839 - val_loss: 0.0425 - val_acc: 0.9857\n",
      "Epoch 4/10\n",
      " - 133s - loss: 0.0402 - acc: 0.9877 - val_loss: 0.0408 - val_acc: 0.9872\n",
      "Epoch 5/10\n",
      " - 133s - loss: 0.0336 - acc: 0.9896 - val_loss: 0.0354 - val_acc: 0.9876\n",
      "Epoch 6/10\n",
      " - 133s - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0316 - val_acc: 0.9888\n",
      "Epoch 7/10\n",
      " - 133s - loss: 0.0232 - acc: 0.9926 - val_loss: 0.0354 - val_acc: 0.9881\n",
      "Epoch 8/10\n",
      " - 133s - loss: 0.0205 - acc: 0.9935 - val_loss: 0.0314 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      " - 133s - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0299 - val_acc: 0.9901\n",
      "Epoch 10/10\n",
      " - 139s - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0311 - val_acc: 0.9901\n",
      "CNN Error: 0.99%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = base_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Convolutional Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "  # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 147s 2ms/step - loss: 0.3893 - acc: 0.8786 - val_loss: 0.0867 - val_acc: 0.9731\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0984 - acc: 0.9697 - val_loss: 0.0556 - val_acc: 0.9820\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.0729 - acc: 0.9777 - val_loss: 0.0377 - val_acc: 0.9875\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.0567 - acc: 0.9821 - val_loss: 0.0329 - val_acc: 0.9892\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 148s 2ms/step - loss: 0.0483 - acc: 0.9849 - val_loss: 0.0288 - val_acc: 0.9898\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.0456 - acc: 0.9858 - val_loss: 0.0329 - val_acc: 0.9900\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.0386 - acc: 0.9879 - val_loss: 0.0312 - val_acc: 0.9888\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.0344 - acc: 0.9892 - val_loss: 0.0314 - val_acc: 0.9892\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 148s 2ms/step - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0271 - val_acc: 0.9911\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.0289 - acc: 0.9909 - val_loss: 0.0230 - val_acc: 0.9920\n",
      "Large CNN Error: 0.80%\n"
     ]
    }
   ],
   "source": [
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
